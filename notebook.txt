Engineering Notebook.

See README for instructions on deployment.

For this assignment, there are no functions at all which can be unit tested, therefore we did not write unit tests.

We demonstrate that the replicas can be on different machines by running the servers on different AWS instances.

Changes from assignment 1:
    We generally use the same structure from the first programming assignment, gRPC version.
    We use gRPC for the abstracted communication between the server replicas and between the client and server.

    Notably, in assignment 1 we used a polling method for client<>client communication.
        This old method worked by using a simple gRPC call, all client<>client chats were stored in the pending
        message database and every 2 seconds the clients would poll the server and get all the chats.

    This wouldn't have worked with the replicas for this assignment, because if all messages are stored in the 
    pending message database and then that server goes down before the poll comes in, then the messages would
    be lost. This would have meant that every chat that gets sent would need to trigger a replica update,
    and could have run into some timing issues.

    So, for this assignment we make the client<>client messages use a streaming RPC, where the chats
    are continuously fed to the client directly rather than going into the pending message database.

    Otherwise, everything is the same as the gRPC implementation of HW1. If it ain't broke don't fix it.

Server:
To make our server 2 fault tolerant, we first spin up 3 replicated instances. Then we use the following methods:
    1. Leader Election: For leader election, we first initialize a heartbeat for each server based off a random integer from 1 to 5 seconds.
    This is in the `start_heartbeat` function. This heartbeat is sent to the other servers. If a heartbeat is not received,
    then the server is assumed to be down and that is noted. The server also validates if the current leader is valid using
    this same aforementioned random interval and with the `valdiate_leader` function. `validate_leader` determines the leader
    by first getting all the servers that are alive and their ids and then choosing the server with the smallest id as the leader.

    2. Replication + Pending Messages Persistence: The database `database` and `active_users` is persisted each time an update is received from a leader or an
    update is sent from the current leader. This is persisted through the `save_db_to_disk` function. Furthermore, whenever an update is
    made to the database or active users, the leader will send the update to all the other servers. This is done through the `send_database_and_users`
    function. This functions sends the latest database and active_users dictionary to the other replicated servers.

Client 
    On the frontend, the client knows the addresses of all three servers.
    When it initially tries to connect, it starts iterating through the addresses, attempting to call the RPC find_leader.
    Whichever server the client connects to will tell the client which is the leader. The client now knows where to open the
    gRPC channel.

    If, the client is operating and any one of the replicas goes down, nothing will happen because the leader is up,
    and the client is connected to the leader.
    But, if the client is operating and the leader goes down, then the client will get a _InactiveRpcError. When this happens,
    we catch the exception and find the new leader again by iterating through the addresses and calling the find_leader RPO.





To demonstrate that we accomplish the two features for this assignment, we do the following demos:
Feature 1: Persistent Storage
    Show for 1 server it can go down / up and messages persist. Show same for 2 servers and 3 servers. 
    Can ask the audience which order to send the servers down to show this is arbitrary.
Feature 2: 2-fault tolerance
    Spin up A, B, C, test (show normal operation on client side). 
    Shut down A, test. 
    Shut down B, test. 
    Restore A, test. 
    Restore B, test. 
    Shut down C, test. 
    Also can ask the audience to choose order of which servers to kill to show this is arbitrary.
